# LayoutLM Model Evaluation Guide

This document describes how to use the annotation files generated by `prepare_annotations.py` as golden labels to evaluate the LayoutLM model's performance on document understanding tasks.

## Overview

The annotation files serve as **ground truth (golden labels)** for evaluating LayoutLM's ability to:
- Extract and classify text entities (NER)
- Understand document layout and structure
- Perform form understanding tasks
- Maintain spatial-textual relationships

## Data Flow

```
Production Model → df_check.csv (predictions) → Annotation Files (ground truth) → Evaluation Metrics
```

## Annotation File Structure

Each annotation file contains:
- **Original predictions**: `pred` column with model's original predictions
- **Confidence scores**: `prob` column with prediction probabilities  
- **Ground truth labels**: `annotator1_label` and `annotator2_label` columns
- **Spatial information**: Bounding box coordinates (`x1`, `y1`, `x2`, `y2`)
- **Text content**: `words` column with actual text
- **Document structure**: Reading order sorted for proper evaluation

## Evaluation Methodology

### 1. Data Preparation

```python
import pandas as pd
from pathlib import Path

def load_annotation_file(file_path):
    """Load completed annotation file with ground truth labels."""
    df = pd.read_excel(file_path, sheet_name='Annotation')
    
    # Use annotator1_label as primary ground truth
    # (can implement inter-annotator agreement checks)
    df['ground_truth'] = df['annotator1_label']
    
    return df[['words', 'pred', 'ground_truth', 'prob', 'x1', 'y1', 'x2', 'y2']]
```

### 2. Standard Evaluation Metrics

#### Entity-Level Evaluation
```python
from sklearn.metrics import classification_report, confusion_matrix

def evaluate_entity_classification(df):
    """Evaluate entity classification performance."""
    
    # Filter out empty/missing annotations
    valid_data = df.dropna(subset=['ground_truth', 'pred'])
    
    # Standard classification metrics
    report = classification_report(
        valid_data['ground_truth'], 
        valid_data['pred'],
        output_dict=True
    )
    
    return {
        'accuracy': report['accuracy'],
        'macro_f1': report['macro avg']['f1-score'],
        'weighted_f1': report['weighted avg']['f1-score'],
        'per_class_metrics': report
    }
```

#### Token-Level Evaluation
```python
def evaluate_token_level(df):
    """Evaluate per-token classification accuracy."""
    
    correct_predictions = (df['pred'] == df['ground_truth']).sum()
    total_tokens = len(df)
    
    return {
        'token_accuracy': correct_predictions / total_tokens,
        'total_tokens': total_tokens,
        'correct_predictions': correct_predictions
    }
```

#### NER-Specific Evaluation (BIO Tags)
```python
from seqeval.metrics import classification_report as seq_report

def evaluate_ner_sequences(annotation_files):
    """Evaluate NER using sequence-level metrics."""
    
    all_true_sequences = []
    all_pred_sequences = []
    
    for file_path in annotation_files:
        df = load_annotation_file(file_path)
        
        # Group by document/image to maintain sequence structure
        true_seq = df['ground_truth'].tolist()
        pred_seq = df['pred'].tolist()
        
        all_true_sequences.append(true_seq)
        all_pred_sequences.append(pred_seq)
    
    # Use seqeval for proper BIO evaluation
    return seq_report(all_true_sequences, all_pred_sequences)
```

### 3. Document Layout Evaluation

#### Spatial Accuracy Assessment
```python
def evaluate_spatial_understanding(df):
    """Evaluate model's understanding of spatial relationships."""
    
    # Group by document layout elements
    layout_elements = ['HEADER', 'FOOTER', 'TITLE', 'PARAGRAPH', 'TABLE_CELL']
    
    layout_accuracy = {}
    for element in layout_elements:
        element_data = df[df['ground_truth'] == element]
        if len(element_data) > 0:
            correct = (element_data['pred'] == element_data['ground_truth']).sum()
            layout_accuracy[element] = correct / len(element_data)
    
    return layout_accuracy
```

#### Reading Order Validation
```python
def evaluate_reading_order(df):
    """Assess if model predictions follow natural reading order."""
    
    # Sort by spatial coordinates (as done in annotation preparation)
    df_sorted = df.sort_values(['y1', 'x1'])
    
    # Compare predicted vs ground truth label transitions
    reading_order_score = calculate_sequence_similarity(
        df_sorted['pred'].tolist(),
        df_sorted['ground_truth'].tolist()
    )
    
    return reading_order_score
```

### 4. Confidence Calibration

```python
def evaluate_confidence_calibration(df):
    """Assess how well prediction probabilities reflect actual accuracy."""
    
    import numpy as np
    from sklearn.calibration import calibration_curve
    
    # Convert ground truth to binary (correct/incorrect)
    correct_predictions = (df['pred'] == df['ground_truth']).astype(int)
    
    # Calibration curve
    fraction_correct, mean_predicted_prob = calibration_curve(
        correct_predictions, df['prob'], n_bins=10
    )
    
    # Expected Calibration Error (ECE)
    ece = np.mean(np.abs(fraction_correct - mean_predicted_prob))
    
    return {
        'ece': ece,
        'calibration_curve': (fraction_correct, mean_predicted_prob)
    }
```

## Comprehensive Evaluation Pipeline

```python
def run_full_evaluation(annotation_directory):
    """Run complete evaluation suite on all annotation files."""
    
    annotation_files = list(Path(annotation_directory).glob("*.xlsx"))
    
    results = {
        'overall_metrics': {},
        'per_file_metrics': {},
        'layout_analysis': {},
        'calibration_analysis': {}
    }
    
    all_data = []
    
    # Process each annotation file
    for file_path in annotation_files:
        df = load_annotation_file(file_path)
        all_data.append(df)
        
        # Per-file evaluation
        file_results = {
            'entity_metrics': evaluate_entity_classification(df),
            'token_metrics': evaluate_token_level(df),
            'spatial_metrics': evaluate_spatial_understanding(df),
            'confidence_metrics': evaluate_confidence_calibration(df)
        }
        
        results['per_file_metrics'][file_path.name] = file_results
    
    # Overall evaluation across all files
    combined_df = pd.concat(all_data, ignore_index=True)
    
    results['overall_metrics'] = {
        'entity_classification': evaluate_entity_classification(combined_df),
        'token_level': evaluate_token_level(combined_df),
        'ner_sequences': evaluate_ner_sequences(annotation_files),
        'layout_understanding': evaluate_spatial_understanding(combined_df),
        'confidence_calibration': evaluate_confidence_calibration(combined_df)
    }
    
    return results
```

## Error Analysis

### 1. Common Error Patterns
```python
def analyze_error_patterns(df):
    """Identify systematic errors in model predictions."""
    
    errors = df[df['pred'] != df['ground_truth']]
    
    # Most common misclassifications
    error_patterns = errors.groupby(['ground_truth', 'pred']).size().sort_values(ascending=False)
    
    # Spatial error analysis
    spatial_errors = analyze_spatial_misclassifications(errors)
    
    # Confidence-based error analysis
    confidence_errors = analyze_confidence_errors(errors)
    
    return {
        'confusion_patterns': error_patterns,
        'spatial_patterns': spatial_errors,
        'confidence_patterns': confidence_errors
    }
```

### 2. Inter-Annotator Agreement
```python
def calculate_inter_annotator_agreement(df):
    """Measure agreement between annotator1 and annotator2."""
    
    from sklearn.metrics import cohen_kappa_score
    
    # Filter rows where both annotators provided labels
    both_labeled = df.dropna(subset=['annotator1_label', 'annotator2_label'])
    
    if len(both_labeled) == 0:
        return None
    
    kappa = cohen_kappa_score(
        both_labeled['annotator1_label'], 
        both_labeled['annotator2_label']
    )
    
    return {
        'kappa_score': kappa,
        'agreement_percentage': (both_labeled['annotator1_label'] == 
                               both_labeled['annotator2_label']).mean(),
        'total_dual_annotations': len(both_labeled)
    }
```

## Reporting and Visualization

### 1. Evaluation Report Generation
```python
def generate_evaluation_report(results, output_path):
    """Generate comprehensive evaluation report."""
    
    report = f"""
# LayoutLM Model Evaluation Report

## Overall Performance
- **Token Accuracy**: {results['overall_metrics']['token_level']['token_accuracy']:.3f}
- **Macro F1-Score**: {results['overall_metrics']['entity_classification']['macro_f1']:.3f}
- **Weighted F1-Score**: {results['overall_metrics']['entity_classification']['weighted_f1']:.3f}

## Layout Understanding
{format_layout_results(results['overall_metrics']['layout_understanding'])}

## Confidence Calibration
- **Expected Calibration Error**: {results['overall_metrics']['confidence_calibration']['ece']:.3f}

## Per-Document Analysis
{format_per_file_results(results['per_file_metrics'])}
"""
    
    with open(output_path, 'w') as f:
        f.write(report)
```

### 2. Visualization Examples
```python
import matplotlib.pyplot as plt
import seaborn as sns

def plot_confusion_matrix(df):
    """Plot confusion matrix for entity classifications."""
    cm = confusion_matrix(df['ground_truth'], df['pred'])
    plt.figure(figsize=(12, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title('Entity Classification Confusion Matrix')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.show()

def plot_calibration_curve(results):
    """Plot reliability diagram for confidence calibration."""
    fraction_correct, mean_predicted_prob = results['overall_metrics']['confidence_calibration']['calibration_curve']
    
    plt.figure(figsize=(8, 6))
    plt.plot([0, 1], [0, 1], 'k--', label='Perfectly calibrated')
    plt.plot(mean_predicted_prob, fraction_correct, 's-', label='Model')
    plt.xlabel('Mean Predicted Probability')
    plt.ylabel('Fraction of Positives')
    plt.title('Calibration Plot')
    plt.legend()
    plt.show()
```

## Usage Example

```python
# Run evaluation on completed annotation files
annotation_dir = "annotation_labels"
results = run_full_evaluation(annotation_dir)

# Generate comprehensive report
generate_evaluation_report(results, "layoutlm_evaluation_report.md")

# Perform error analysis
all_data = []
for file_path in Path(annotation_dir).glob("*.xlsx"):
    all_data.append(load_annotation_file(file_path))

combined_df = pd.concat(all_data, ignore_index=True)
error_analysis = analyze_error_patterns(combined_df)

print("Top 10 Error Patterns:")
print(error_analysis['confusion_patterns'].head(10))
```

## Key Benefits of This Evaluation Approach

1. **Comprehensive Coverage**: Evaluates both entity recognition and document understanding
2. **Spatial Awareness**: Considers layout and reading order in evaluation
3. **Quality Assurance**: Inter-annotator agreement ensures reliable ground truth
4. **Confidence Assessment**: Evaluates model uncertainty quantification
5. **Error Analysis**: Identifies systematic weaknesses for model improvement
6. **Standardized Pipeline**: Reproducible evaluation across different document types

This evaluation framework leverages the high-quality annotation files produced by the enhanced preparation pipeline to provide thorough, reliable assessment of LayoutLM model performance.