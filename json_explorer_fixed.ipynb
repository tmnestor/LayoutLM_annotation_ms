{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JSON Explorer Notebook\n",
    "\n",
    "This notebook uses the my_json_explorer.py script to analyze JSON files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Script Functions\n",
    "\n",
    "First, let's import the functions from our script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import functions from my_json_explorer.py\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.getcwd())\n",
    "from my_json_explorer import explore_json, extract_text_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Explore a JSON File\n",
    "\n",
    "Now we can load and explore a JSON file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def load_json_file(file_path):\n",
    "    \"\"\"Load a JSON file and return its contents.\"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your JSON file path\n",
    "json_file_path = \"path/to/your/file.json\"\n",
    "data = load_json_file(json_file_path)\n",
    "\n",
    "print(f\"Loaded JSON file: {json_file_path}\")\n",
    "print(f\"Top level type: {type(data).__name__}\")\n",
    "\n",
    "if isinstance(data, dict):\n",
    "    print(f\"Keys: {', '.join(list(data.keys()))}\")\n",
    "elif isinstance(data, list):\n",
    "    print(f\"List length: {len(data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Structure\n",
    "\n",
    "Explore the structure of the JSON data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore with a depth of 3 levels\n",
    "# The explore_json function prints directly rather than returning values\n",
    "print(\"Structure exploration:\")\n",
    "explore_json(data, max_depth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Text Values\n",
    "\n",
    "Extract text values from the \"extracted_lines\" field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_values = extract_text_values(data)\n",
    "\n",
    "if text_values:\n",
    "    print(f\"Found {len(text_values)} text values:\")\n",
    "    for i, text in enumerate(text_values[:10]):\n",
    "        print(f\"{i+1}. {text}\")\n",
    "    if len(text_values) > 10:\n",
    "        print(f\"... and {len(text_values) - 10} more\")\n",
    "    \n",
    "    # Create a DataFrame for better visualization\n",
    "    df = pd.DataFrame({\"text\": text_values})\n",
    "    df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call my_json_explorer.py Directly\n",
    "\n",
    "You can also call the script directly using the shell magic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your JSON file path\n",
    "file_path = \"path/to/your/file.json\"\n",
    "!python my_json_explorer.py {file_path} --extract-text --depth 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom JSON Path Query\n",
    "\n",
    "Extract data using a custom path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_json(data, path_str):\n",
    "    \"\"\"Extract data from a nested JSON structure using a dot-notation path.\"\"\"\n",
    "    result = data\n",
    "    path = path_str.split('.')\n",
    "    \n",
    "    for key in path:\n",
    "        # Handle array indexing with [n]\n",
    "        if '[' in key and ']' in key:\n",
    "            base_key, idx_str = key.split('[', 1)\n",
    "            idx = int(idx_str.rstrip(']'))\n",
    "            \n",
    "            if base_key:\n",
    "                result = result[base_key][idx]\n",
    "            else:\n",
    "                result = result[idx]\n",
    "        else:\n",
    "            result = result[key]\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Example: query_json(data, \"extracted_lines.0.text\")\n",
    "path_to_query = \"extracted_lines\"\n",
    "try:\n",
    "    result = query_json(data, path_to_query)\n",
    "    if isinstance(result, (dict, list)):\n",
    "        from pprint import pprint\n",
    "        pprint(result, depth=2)\n",
    "    else:\n",
    "        print(result)\n",
    "except (KeyError, IndexError, TypeError) as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the Structure of df_check.csv Files\n",
    "\n",
    "This can help diagnose issues with the prepare_annotations.py script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Replace with your actual path\n",
    "cases_dir = Path(\"/Users/tod/Desktop/LayoutLM_annotation_ms/du_cases\")\n",
    "case_dirs = [d for d in cases_dir.iterdir() if d.is_dir()]\n",
    "\n",
    "csv_files = []\n",
    "for case_path in case_dirs:\n",
    "    csv_path = case_path / \"processing\" / \"form-recogniser\" / \"df_check.csv\"\n",
    "    if csv_path.exists():\n",
    "        csv_files.append((case_path.name, csv_path))\n",
    "\n",
    "print(f\"Found {len(csv_files)} df_check.csv files\")\n",
    "\n",
    "# Analyze the first file to understand its structure\n",
    "if csv_files:\n",
    "    case_id, csv_path = csv_files[0]\n",
    "    print(f\"\\nAnalyzing file for case: {case_id}\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    print(f\"Columns: {', '.join(df.columns)}\")\n",
    "    print(f\"Number of rows: {len(df)}\")\n",
    "    \n",
    "    # Check what values are in the image_id column\n",
    "    if 'image_id' in df.columns:\n",
    "        unique_ids = df['image_id'].unique()\n",
    "        print(f\"\\nUnique image_id values ({len(unique_ids)}):\\n{unique_ids}\")\n",
    "    \n",
    "    # Display the first few rows\n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra Analysis: Compare image_id values with page_id values from annotation_images.csv\n",
    "\n",
    "# Load annotation_images.csv\n",
    "annotation_images_path = Path(\"/Users/tod/Desktop/LayoutLM_annotation_ms/annotation_labels/annotation_images.csv\")\n",
    "if annotation_images_path.exists():\n",
    "    # Read the CSV\n",
    "    annotation_df = pd.read_csv(annotation_images_path)\n",
    "    print(f\"Loaded annotation_images.csv with {len(annotation_df)} rows\")\n",
    "    \n",
    "    # Extract unique page_id values\n",
    "    page_ids = set(annotation_df['page_id']) if 'page_id' in annotation_df.columns else set()\n",
    "    print(f\"Found {len(page_ids)} unique page_id values\")\n",
    "    \n",
    "    # Compare with image_id values in df_check.csv\n",
    "    if csv_files and 'image_id' in df.columns:\n",
    "        image_ids = set(df['image_id'])\n",
    "        print(f\"Found {len(image_ids)} unique image_id values in the first df_check.csv\")\n",
    "        \n",
    "        # Find matches and mismatches\n",
    "        matches = page_ids.intersection(image_ids)\n",
    "        missing_in_df = page_ids - image_ids\n",
    "        extra_in_df = image_ids - page_ids\n",
    "        \n",
    "        print(f\"\\nMatches: {len(matches)} page_ids match image_ids\")\n",
    "        print(f\"Missing: {len(missing_in_df)} page_ids not found in image_ids\")\n",
    "        print(f\"Extra: {len(extra_in_df)} image_ids not found in page_ids\")\n",
    "        \n",
    "        # Show examples of mismatches if any\n",
    "        if missing_in_df:\n",
    "            print(\"\\nExamples of page_ids not found in image_ids:\")\n",
    "            for pid in list(missing_in_df)[:5]:\n",
    "                print(f\"  - '{pid}'\")\n",
    "        \n",
    "        if extra_in_df:\n",
    "            print(\"\\nExamples of image_ids not found in page_ids:\")\n",
    "            for iid in list(extra_in_df)[:5]:\n",
    "                print(f\"  - '{iid}'\")\n",
    "else:\n",
    "    print(f\"Warning: Could not find annotation_images.csv at {annotation_images_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}