{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JSON Explorer Notebook\n",
    "\n",
    "This notebook uses the my_json_explorer.py script to analyze JSON files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Script Functions\n",
    "\n",
    "First, let's import the functions from our script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import functions from my_json_explorer.py\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.getcwd())\n",
    "from my_json_explorer import explore_json, extract_text_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Explore a JSON File\n",
    "\n",
    "Now we can load and explore a JSON file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def load_json_file(file_path):\n",
    "    \"\"\"Load a JSON file and return its contents.\"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your JSON file path\n",
    "json_file_path = \"path/to/your/file.json\"\n",
    "data = load_json_file(json_file_path)\n",
    "\n",
    "print(f\"Loaded JSON file: {json_file_path}\")\n",
    "print(f\"Top level type: {type(data).__name__}\")\n",
    "\n",
    "if isinstance(data, dict):\n",
    "    print(f\"Keys: {', '.join(list(data.keys()))}\")\n",
    "elif isinstance(data, list):\n",
    "    print(f\"List length: {len(data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Structure\n",
    "\n",
    "Explore the structure of the JSON data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore with a depth of 3 levels\n",
    "# The explore_json function prints directly rather than returning values\n",
    "print(\"Structure exploration:\")\n",
    "explore_json(data, max_depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Text Values\n",
    "\n",
    "Extract text values from the \"extracted_lines\" field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_values = extract_text_values(data)\n",
    "\n",
    "if text_values:\n",
    "    print(f\"Found {len(text_values)} text values:\")\n",
    "    for i, text in enumerate(text_values[:10]):\n",
    "        print(f\"{i+1}. {text}\")\n",
    "    if len(text_values) > 10:\n",
    "        print(f\"... and {len(text_values) - 10} more\")\n",
    "    \n",
    "    # Create a DataFrame for better visualization\n",
    "    df = pd.DataFrame({\"text\": text_values})\n",
    "    df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call my_json_explorer.py Directly\n",
    "\n",
    "You can also call the script directly using the shell magic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your JSON file path\n",
    "file_path = \"path/to/your/file.json\"\n",
    "!python my_json_explorer.py {file_path} --extract-text --depth 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom JSON Path Query\n",
    "\n",
    "Extract data using a custom path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_json(data, path_str):\n",
    "    \"\"\"Extract data from a nested JSON structure using a dot-notation path.\"\"\"\n",
    "    result = data\n",
    "    path = path_str.split('.')\n",
    "    \n",
    "    for key in path:\n",
    "        # Handle array indexing with [n]\n",
    "        if '[' in key and ']' in key:\n",
    "            base_key, idx_str = key.split('[', 1)\n",
    "            idx = int(idx_str.rstrip(']'))\n",
    "            \n",
    "            if base_key:\n",
    "                result = result[base_key][idx]\n",
    "            else:\n",
    "                result = result[idx]\n",
    "        else:\n",
    "            result = result[key]\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Example: query_json(data, \"extracted_lines.0.text\")\n",
    "path_to_query = \"extracted_lines\"\n",
    "try:\n",
    "    result = query_json(data, path_to_query)\n",
    "    if isinstance(result, (dict, list)):\n",
    "        from pprint import pprint\n",
    "        pprint(result, depth=2)\n",
    "    else:\n",
    "        print(result)\n",
    "except (KeyError, IndexError, TypeError) as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Analyze the Structure of df_check.csv Files\n\nThis can help diagnose issues with the prepare_annotations.py script:",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nfrom pathlib import Path\n\n# Replace with your actual path - make sure it's a Path object\ncases_dir = Path(\"/Users/tod/Desktop/LayoutLM_annotation_ms/du_cases\")\n\n# Use pathlib methods\ncase_dirs = [d for d in cases_dir.iterdir() if d.is_dir()]\n\ncsv_files = []\nfor case_path in case_dirs:\n    csv_path = case_path / \"processing\" / \"form-recogniser\" / \"df_check.csv\"\n    if csv_path.exists():\n        csv_files.append((case_path.name, csv_path))\n\nprint(f\"Found {len(csv_files)} df_check.csv files\")\n\n# Analyze the first file to understand its structure\nif csv_files:\n    case_id, csv_path = csv_files[0]\n    print(f\"\\nAnalyzing file for case: {case_id}\")\n    df = pd.read_csv(csv_path)\n    \n    print(f\"Columns: {', '.join(df.columns)}\")\n    print(f\"Number of rows: {len(df)}\")\n    \n    # Check what values are in the image_id column\n    if 'image_id' in df.columns:\n        unique_ids = df['image_id'].unique()\n        print(f\"\\nUnique image_id values ({len(unique_ids)}):\\n{unique_ids}\")\n    \n    # Display the first few rows\n    print(\"\\nFirst 5 rows:\")\n    df.head()"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}