# NER Evaluation Script for LayoutLM

This document provides comprehensive documentation for `evaluate_ner.py`, a specialized script for evaluating Named Entity Recognition (NER) performance using annotation files generated by the LayoutLM annotation preparation pipeline.

## Overview

The `evaluate_ner.py` script focuses specifically on evaluating the **"Extract and classify text entities (NER)"** capability of LayoutLM models. It uses completed annotation files as ground truth to compute comprehensive NER performance metrics.

### Key Features

- **Configurable Annotator Selection**: Choose between `annotator1` or `annotator2` as ground truth
- **Smart Label Normalization**: Handles BIO model predictions vs flat human annotations
- **Dual Evaluation Modes**: Both flat and BIO label comparison for comprehensive analysis
- **Invoice/Receipt Entity Support**: Automatically extracts 58+ specialized entity labels
- **Prediction Decoding**: Converts numeric model predictions to meaningful entity labels
- **Multiple Evaluation Metrics**: Entity-level, token-level, and sequence-level NER metrics
- **Error Pattern Analysis**: Identifies systematic misclassification patterns
- **Inter-Annotator Agreement**: Measures agreement between annotators using Cohen's kappa
- **Comprehensive Reporting**: Generates both Markdown and JSON reports
- **Robust Error Handling**: Handles missing data and incomplete annotations gracefully

## Installation Requirements

### Python Dependencies

```bash
# Core dependencies
pip install pandas numpy scikit-learn openpyxl

# Optional but recommended for proper NER sequence evaluation
pip install seqeval
```

### Conda Environment (Recommended)

```bash
# If using the project's conda environment
source /opt/homebrew/Caskroom/miniforge/base/etc/profile.d/conda.sh && conda activate internvl_env

# Install openpyxl if not available (required for reading Excel files)
conda install openpyxl -y
```

**Note**: `openpyxl` is required for reading Excel annotation files. If you encounter the error "Missing optional dependency 'openpyxl'", install it using the command above.

## Input Requirements

### Annotation File Structure

The script expects Excel (.xlsx) annotation files with the following structure:

**Required Columns:**
- `words`: Text tokens/words
- `pred`: Model predictions (numeric indices 0-57 that map to entity labels)
- `prob`: Prediction confidence scores
- `annotator1_label`: Ground truth labels from first annotator (flat entity types like "ADDRESS", "MONEY", "ORG")
- `annotator2_label`: Ground truth labels from second annotator (optional)

**Required Sheets:**
- `Annotation`: Main data with predictions and annotations
- `Validation`: Contains the 58+ invoice/receipt entity labels for prediction decoding

**Optional Columns:**
- `x1`, `y1`, `x2`, `y2`: Bounding box coordinates
- Additional metadata columns

### Directory Structure

```
annotation_labels/
├── 1-1AAAAAAA_1234567890_01_0.xlsx
├── 1-1AAAAAAA_1234567890_01_1.xlsx
├── 1-1BBBBBBB_2345678901_01_0.xlsx
└── ...
```

## Usage

### Basic Usage

```bash
# Evaluate using annotator1 as ground truth (default)
python evaluate_ner.py annotation_labels

# Evaluate using annotator2 as ground truth
python evaluate_ner.py annotation_labels --annotator annotator2
```

### Advanced Usage

```bash
# Custom output directory and report naming
python evaluate_ner.py annotation_labels \
    --annotator annotator1 \
    --output-dir results \
    --report-name layoutlm_eval_v1

# Verbose logging
python evaluate_ner.py annotation_labels --annotator annotator1 --verbose

# Quiet mode (errors and warnings only)
python evaluate_ner.py annotation_labels --annotator annotator2 --quiet
```

## Command Line Arguments

| Argument | Type | Default | Description |
|----------|------|---------|-------------|
| `annotation_dir` | positional | - | Directory containing .xlsx annotation files |
| `--annotator` | choice | `annotator1` | Which annotator to use as ground truth (`annotator1` or `annotator2`) |
| `--output-dir` | string | `reports` | Directory for output reports |
| `--report-name` | string | `ner_evaluation_report` | Base name for output files |
| `--verbose` | flag | False | Enable verbose logging |
| `--quiet` | flag | False | Reduce output to warnings and errors only |

## Output Files

The script generates two types of output files:

### 1. Markdown Report (`{report_name}_{annotator}.md`)

A human-readable evaluation report containing:
- Evaluation summary and configuration
- Overall performance metrics
- Error analysis with confusion patterns
- Inter-annotator agreement (if available)
- Per-file performance breakdown

### 2. JSON Results (`{report_name}_{annotator}.json`)

Machine-readable detailed results including:
- Complete evaluation metrics
- Per-class performance statistics
- Detailed error patterns
- Raw confusion matrices
- Per-file breakdowns

## Evaluation Metrics

### 1. Label Processing and Normalization

**Smart Label Handling:**
The script automatically handles the mismatch between model predictions and human annotations:

- **Model Predictions**: Numeric indices (0, 1, 2...) decoded to BIO format (B-ADDRESS, I-ADDRESS, B-MONEY...)
- **Human Annotations**: Flat entity types (ADDRESS, MONEY, ORG...)
- **Automatic Conversion**: Creates both flat and BIO versions for proper comparison

### 2. Token-Level Evaluation (Flat Label Comparison)

**Primary Token Metrics:**
- **Token Accuracy**: Percentage of correctly classified tokens using flat label comparison
- **Total Tokens**: Number of tokens evaluated
- **Correct Predictions**: Number of correctly predicted tokens

*Uses flat labels: Model "B-MONEY" → "MONEY" vs Human "MONEY"*

### 3. Entity Classification (Dual Mode)

**Flat Label Classification:**
- **Accuracy**: Overall classification accuracy using stripped entity types
- **Macro F1-Score**: Unweighted average F1 across all entity types
- **Weighted F1-Score**: Weighted average F1 (by support)
- **Per-Class Metrics**: Individual performance for each entity type

**BIO Format Classification:**
- **Accuracy**: Classification accuracy using full BIO tags
- **Macro/Weighted F1**: F1 scores with proper BIO sequence handling
- **Entity Boundary Detection**: Measures complete entity extraction accuracy

### 4. NER Sequence Evaluation (with seqeval)

**Sequence-Level Metrics:**
- **Sequence Accuracy**: Strict sequence matching accuracy using BIO format
- **Sequence F1-Score**: F1 score for complete entity sequences
- **Detailed Entity Report**: Performance breakdown by entity type using proper BIO evaluation

*Note: Requires `seqeval` package for proper BIO tag handling. Falls back to token-level evaluation if unavailable.*

### 5. Error Pattern Analysis

**Error Analysis Metrics:**
- **Total Errors**: Count of misclassified tokens
- **Error Rate**: Percentage of incorrect predictions
- **Confusion Patterns**: Most common misclassification pairs
- **Most Confused Entities**: Entity types with highest error rates

### 6. Inter-Annotator Agreement

**Agreement Metrics:**
- **Cohen's Kappa**: Statistical measure of inter-rater agreement
- **Agreement Percentage**: Simple percentage agreement
- **Total Dual Annotations**: Number of tokens annotated by both annotators

## Example Output

### Console Output
```
=== NER Evaluation Summary ===
Ground truth annotator: annotator1
Files evaluated: 25
Label mapping: 58 invoice/receipt entities
Total tokens: 1,247
Token accuracy: 0.876
Entity F1 (flat): 0.823
Entity F1 (BIO): 0.791

Reports saved:
  - Markdown: reports/ner_evaluation_report_annotator1.md
  - JSON: reports/ner_evaluation_report_annotator1.json
```

### Sample Markdown Report Excerpt
```markdown
# LayoutLM NER Evaluation Report

## Evaluation Summary
- **Total Files Evaluated**: 25
- **Annotation Directory**: annotation_labels
- **Ground Truth Annotator**: annotator1
- **Label Mapping Found**: True
- **Total Entity Labels**: 58

## Overall Performance Metrics

### Token-Level Performance (Flat Label Comparison)
- **Token Accuracy**: 0.876
- **Total Tokens**: 1,247
- **Correct Predictions**: 1,093

### Entity Classification (Flat Labels)
- **Accuracy**: 0.876
- **Macro F1-Score**: 0.791
- **Weighted F1-Score**: 0.823

### Entity Classification (BIO Format)
- **Accuracy**: 0.842
- **Macro F1-Score**: 0.756
- **Weighted F1-Score**: 0.798

### Top Confusion Patterns

| True Label → Predicted Label | Count |
|-------------------------------|-------|
| DATE → TIME | 23 |
| MONEY → QUANTITY | 18 |
| ORG → PER | 12 |
```

## Understanding Flat vs BIO Label Comparison

### Why Two Evaluation Modes?

The script provides both **flat** and **BIO** evaluation because your setup has a fundamental mismatch:

- **LayoutLM Model**: Outputs BIO-tagged predictions (B-ADDRESS, I-ADDRESS, B-MONEY, I-MONEY...)
- **Human Annotators**: Provide flat entity labels (ADDRESS, MONEY, ORG...)

### How the Script Handles This

**1. Prediction Decoding:**
```
Model Output: [0, 1, 2, 3] 
↓ (decode using validation vocabulary)
BIO Format: [B-ADDRESS, I-ADDRESS, B-MONEY, I-MONEY]
↓ (strip prefixes for flat comparison)
Flat Format: [ADDRESS, ADDRESS, MONEY, MONEY]
```

**2. Human Label Processing:**
```
Human Input: [ADDRESS, ADDRESS, MONEY, MONEY]
↓ (convert contiguous sequences to BIO)
BIO Format: [B-ADDRESS, I-ADDRESS, B-MONEY, I-MONEY]
```

### Interpretation Guide

**Flat Label Results (Primary):**
- **Best for business evaluation**: "Does the model extract the right entity types?"
- **Token accuracy**: Percentage of correctly identified entity types
- **Error patterns**: "MONEY → QUANTITY" (18 cases)

**BIO Format Results (Technical):**
- **Best for model debugging**: "Does the model understand entity boundaries?"
- **Entity completeness**: Are complete entities extracted properly?
- **Boundary errors**: Missing B- tags, incorrect I- continuations

### Example Scenarios

**Scenario 1: Good Flat, Poor BIO**
```
Text: "$ 1,250.50"
Human: MONEY MONEY
Model: B-MONEY B-MONEY  (should be B-MONEY I-MONEY)

Flat accuracy: 100% (both tokens correctly identified as MONEY)
BIO accuracy: 50% (entity boundary error - should be I-MONEY for second token)
```

**Scenario 2: Perfect Match**
```
Text: "ABC Corp"  
Human: ORG ORG → B-ORG I-ORG (automatic conversion)
Model: B-ORG I-ORG

Flat accuracy: 100%
BIO accuracy: 100%
```

### Which Metric to Focus On

- **For business decisions**: Use **flat label results** (primary metric)
- **For model debugging**: Use **BIO format results** (technical analysis)
- **For production readiness**: Both should be high

## Best Practices

### 1. Annotation Quality
- Ensure annotations are complete before evaluation
- Use consistent labeling schemes across annotators
- Address any systematic disagreements between annotators

### 2. Comparative Analysis
```bash
# Compare both annotators as ground truth
python evaluate_ner.py annotation_labels --annotator annotator1 --report-name comparison_ann1
python evaluate_ner.py annotation_labels --annotator annotator2 --report-name comparison_ann2
```

### 3. Iterative Improvement
- Use error pattern analysis to identify model weaknesses
- Focus training data collection on most confused entity types
- Monitor inter-annotator agreement to ensure data quality

### 4. Performance Tracking
```bash
# Track performance over time with dated reports
python evaluate_ner.py annotation_labels --report-name "eval_$(date +%Y%m%d)"
```

## Integration with Annotation Pipeline

### Workflow Integration

1. **Preparation**: Use `prepare_annotations.py` to generate annotation files
2. **Annotation**: Human annotators complete the labeling process
3. **Evaluation**: Use `evaluate_ner.py` to assess model performance
4. **Analysis**: Review reports to identify improvement opportunities

### Quality Assurance Pipeline

```bash
# Step 1: Generate annotation files
python prepare_annotations.py --cases-dir du_cases --labels-dir annotation_labels

# Step 2: After annotation completion, evaluate with both annotators
python evaluate_ner.py annotation_labels --annotator annotator1 --report-name qa_ann1
python evaluate_ner.py annotation_labels --annotator annotator2 --report-name qa_ann2

# Step 3: Compare results and identify quality issues
```

## Troubleshooting

### Common Issues

**1. No annotation files found**
```
ERROR: No .xlsx files found in annotation_labels
```
**Solution**: Verify the directory path and ensure annotation files exist.

**2. Missing annotator columns**
```
WARNING: Column annotator1_label not found in file.xlsx
```
**Solution**: Ensure annotation files were generated with the correct structure and annotators have completed their work.

**3. seqeval import error**
```
WARNING: seqeval not available - falling back to token-level evaluation
```
**Solution**: Install seqeval for proper NER sequence evaluation: `pip install seqeval`

**4. Empty results**
```
WARNING: No valid ground truth annotations found
```
**Solution**: Check that the selected annotator has completed annotations in the files.

**5. Label mapping extraction failure**
```
WARNING: Could not extract label vocabulary from annotation files
```
**Solution**: Ensure annotation files have a 'Validation' sheet with 'Label Options' column containing the entity vocabulary.

**6. Prediction decoding errors**
```
WARNING: No label mapping found - predictions will not be decoded
```
**Solution**: Verify the Validation sheet structure and that numeric predictions are within the valid range (0 to number of labels - 1).

### Debugging Options

```bash
# Enable verbose logging for detailed debugging
python evaluate_ner.py annotation_labels --verbose

# Check specific annotator availability
python evaluate_ner.py annotation_labels --annotator annotator2 --verbose
```

## Performance Considerations

- **Memory Usage**: Large annotation datasets may require significant memory
- **Processing Time**: Evaluation time scales with number of files and tokens
- **Disk Space**: JSON reports can be large for comprehensive evaluations

## Invoice/Receipt Entity Types

The script automatically recognizes and evaluates performance on specialized document understanding entities:

### Financial Entities
- `MONEY`, `B-MONEY`, `I-MONEY`: Invoice amounts, totals, prices
- `PERCENT`, `B-PERCENT`, `I-PERCENT`: Tax rates, discounts
- `QUANTITY`, `B-QUANTITY`, `I-QUANTITY`: Item quantities
- `CARDINAL`, `B-CARDINAL`, `I-CARDINAL`: Numbers, counts

### Business Entities  
- `ORG`, `B-ORG`, `I-ORG`: Company names, vendors
- `PERSON`, `B-PERSON`, `I-PERSON`: Contact names
- `ADDRESS`, `B-ADDRESS`, `I-ADDRESS`: Billing/shipping addresses
- `PHONE`, `B-PHONE`, `I-PHONE`: Contact numbers
- `EMAIL`, `B-EMAIL`, `I-EMAIL`: Email addresses

### Temporal Entities
- `DATE`, `B-DATE`, `I-DATE`: Invoice dates, due dates
- `TIME`, `B-TIME`, `I-TIME`: Timestamps

### Document Structure
- `HEADER`: Document headers
- `FOOTER`: Document footers  
- `TABLE_HEADER`: Table column headers
- `TABLE_CELL`: Table data cells
- `TITLE`: Document titles
- `SUBTITLE`: Section headings

This specialized vocabulary enables precise evaluation of invoice and receipt processing capabilities, with meaningful business-relevant error analysis.

## Future Enhancements

Potential improvements for future versions:
- Support for additional evaluation metrics (entity-level precision/recall)
- Visualization generation (confusion matrices, performance charts)
- Batch evaluation across multiple annotation directories
- Integration with MLflow or other experiment tracking systems
- Support for custom entity hierarchies and evaluation schemes
- Document-level aggregation metrics (per-invoice accuracy)

## Related Documentation

- [`README.md`](README.md) - Main project documentation
- [`README_python_script_notebook_integration.md`](README_python_script_notebook_integration.md) - Python/Jupyter integration guide
- [`evaluation.md`](evaluation.md) - Comprehensive evaluation methodology guide

## Support and Contribution

For issues, questions, or contributions related to the NER evaluation script:
1. Check this documentation first
2. Review error messages and logs with `--verbose` flag
3. Ensure all dependencies are properly installed
4. Verify annotation file structure and completeness